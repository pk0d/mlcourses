{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering Tutorial with Titanic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/kernels/scriptcontent/13445201/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b92c0af57d3a181c7b72fce0a2e5b20164bd518b"
   },
   "source": [
    "> ## **0. Introduction**\n",
    "\n",
    "> **tl;dr** Created many features by grouping and used a simple ensemble model.\n",
    "\n",
    "> I decided to write this kernel because **Titanic: Machine Learning from Disaster** is one of my favorite competitions on Kaggle. This is a basic level kernel which focuses on **Exploratory Data Analysis** and **Feature Engineering**. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a relatively short kernel compared to others. I hope this will be a guide for starters and inspire them with Feature Engineering ideas.\n",
    "\n",
    "> **Titanic: Machine Learning from Disaster** is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are features still waiting to be discovered. \n",
    "\n",
    "> This kernel has **3** main parts; **Exploratory Data Analysis**, **Feature Engineering** and **Machine Learning**, and it can achieve top **5%** LB score with a simple tuned Random Forest Classifier. It takes 30 seconds to run whole notebook. I have been revisiting this kernel from time to time. If you have any idea that might improve this kernel, please be sure to comment or fork and experiment as you like.\n",
    "\n",
    "> **TODO;**\n",
    "* The continuous features (`Fare` and `Age`) are binned based on their quantiles. In order to increase the information gain via binning, entropy based binning can be implemented. This way the most homogeneous groups in those features can be revealed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    ">> ### **0.1. Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "import string\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7dc1cb7b8813f315a8b30fd636b3d46b4270496"
   },
   "source": [
    ">> ### **0.2. Loading the Dataset**\n",
    "* Training set has **891** rows and test set has **418** rows.\n",
    "* Training set have **12** features and test set have **11** features.\n",
    "* One extra feature in the training set is `Survived` feature, which is the class of a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "467443fda7135a8ce89c4d537da3f3a8546e2384"
   },
   "outputs": [],
   "source": [
    "def concat_df(train_data, test_data):\n",
    "    # Returns a concatenated df of training and test set on axis 0\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n",
    "\n",
    "def divide_df(all_data):\n",
    "    # Returns divided dfs of training and test set\n",
    "    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_all = concat_df(df_train, df_test)\n",
    "\n",
    "df_train.name = 'Training Set'\n",
    "df_test.name = 'Test Set'\n",
    "df_all.name = 'All Set' \n",
    "\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "print('Number of Training Examples = {}'.format(df_train.shape[0]))\n",
    "print('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\n",
    "print('Training X Shape = {}'.format(df_train.shape))\n",
    "print('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\n",
    "print('Test X Shape = {}'.format(df_test.shape))\n",
    "print('Test y Shape = {}\\n'.format(df_test.shape[0]))\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfabd70ff1cbd50e3107727e5bb630aa59110d83"
   },
   "source": [
    "> ## **1. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff42f5c4bad84e23f78fa56b9e1a72abf577942d"
   },
   "source": [
    ">> ### **1.1. Overview**\n",
    "* `PassengerId` feature is the unique id of the row and it doesn't have any effect on `Survived`.\n",
    "* `Survived` feature is binary (**0** or **1**); \n",
    "    - **1 = Survived**\n",
    "    - **0 = Not Survived**\n",
    "* `Pclass` (Passenger Class) feature is the socio-economic status of the passenger. It is a categorical ordinal feature which has **3** unique values (**1**,  **2 **or **3**);\n",
    "    - **1 = Upper Class**\n",
    "    - **2 = Middle Class**\n",
    "    - **3 = Lower Class**\n",
    "* `Name`, `Sex` and `Age` features are self-explanatory.\n",
    "* `SibSp` feature is the total number of the passengers' siblings and spouse.\n",
    "* `Parch` feature is the total number of the passengers' parents and children.\n",
    "* `Ticket` feature is the ticket number of the passenger.\n",
    "* `Fare` feature is the passenger fare.\n",
    "* `Cabin` feature is the cabin number of the passenger.\n",
    "* `Embarked` is port of embarkation. It is a categorical feature and it has **3** unique values (**C**, **Q** or **S**);\n",
    "    - **C = Cherbourg**\n",
    "    - **Q = Queenstown**\n",
    "    - **S = Southampton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f02f321f8fd8b8c7c2a4aedb36ebe868ae51004e"
   },
   "outputs": [],
   "source": [
    "print(df_train.info())\n",
    "df_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "851ccf74127831d31ea0d7273b686f9a7cf20eee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_test.info())\n",
    "df_test.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3db8a853b0b33256f1b08f77d7edf0e9d1737d62"
   },
   "source": [
    ">> ### **1.2. Dealing with Missing Values**\n",
    "As seen from below, there are some columns have missing values. The `display_missing` function shows the sum of missing values in each column in both training and test set.\n",
    "* Training set have missing values in `Age`, `Cabin` and `Embarked` columns\n",
    "* Test set have missing values in `Age`, `Cabin` and `Fare` columns\n",
    "\n",
    ">> It is convenient to merge training and test set before dealing with missing values, otherwise the filled data tends to overfit training set. The amount of missing values in `Age`, `Embarked` and `Fare` features are relatively smaller compared to the total samples, but roughly **80%** of the `Cabin` feature is missing in both training and test set. Missing values in `Age`, `Embarked` and `Fare` features can be filled with descriptive statistical measures, but that wouldn't work for `Cabin` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4e8f7b72e2bd165cafa71d67c95f008e7c6101d"
   },
   "outputs": [],
   "source": [
    "def display_missing(df):    \n",
    "    for col in df.columns.tolist():          \n",
    "        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n",
    "    print('\\n')\n",
    "    \n",
    "for df in dfs:\n",
    "    print('{}'.format(df.name))\n",
    "    display_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.2.1. Age**\n",
    "Missing values in `Age` feature are filled with the median age, but using the median age of the whole data set is not a good choice. Median age of a group is much better because the new values would be more informative. Median age of `Pclass` groups is the best choice because of its high correlation with `Age` **(0.408106)** and `Survived` **(0.338481)** features, and it has only **3** unique values. It also makes more sense to group ages by passenger classes instead of other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_all_corr[df_all_corr['Feature 1'] == 'Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> In order to be more accurate, `Sex` feature is used as the second level of `groupby`. As seen from below, the `Pclass` and `Sex` groups have distinct median `Age` values. When passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median `Age` compared to males. The median ages below are used for filling the missing values in `Age` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n",
    "\n",
    "for pclass in range(1, 4):\n",
    "    for sex in ['female', 'male']:\n",
    "        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\n",
    "print('Median age of all passengers: {}'.format(df_all['Age'].median()))\n",
    "\n",
    "df_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.2.2. Embarked**\n",
    "`Embarked` is a categorical feature and there are only **2** missing values in the whole dataset. Both of those passengers are female, upper class and they have the same ticket number. This means, they know each other and embarked from the same port together. The mode `Embarked` value for an upper class female passenger is **C (Cherbourg)**, but this doesn't necessarily mean that they embarked from that port. \n",
    "\n",
    ">>> When I googled **Stone, Mrs. George Nelson (Martha Evelyn)**, I found that she embarked from **S (Southampton)** with her maid **Amelie Icard**, in this page [Martha Evelyn Stone: Titanic Survivor](https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html). \n",
    "\n",
    ">>> *Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.* This was the information we were looking for and case closed for `Embarked` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Embarked'] = df_all['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.2.3. Fare**\n",
    "There is only one passenger with missing `Fare` value. We can assume that `Fare` is related to family size (`Parch` and `SibSp`) and `Pclass` features. Median `Fare` value of a male with a third class ticket and no family is a logical choice to fill the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "df_all['Fare'] = df_all['Fare'].fillna(med_fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.2.4. Cabin**\n",
    ">>> The `Cabin` feature is little bit tricky and it needs further exploration. The large portion of the `Cabin` feature is missing and the feature itself can't be ignored completely because some cabins might have higher survival rates. It turns out to be the first letter of the `Cabin` values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.\n",
    "![alt text](https://vignette.wikia.nocookie.net/titanic/images/f/f9/Titanic_side_plan.png/revision/latest?cb=20180322183733)\n",
    "* On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z**, but only the **T** cabin is present in the dataset.\n",
    "* **A**, **B** and **C** decks were only for 1st class passengers.\n",
    "* **D** and **E** decks were for all classes.\n",
    "* **F** and **G** decks were for both 2nd and 3rd class passengers.\n",
    "* From going **A** to **G**, the distance to the staircase increases, which might be a factor of survival. If it is true, then the new `Deck` feature is ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\n",
    "df_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "\n",
    "df_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n",
    "                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n",
    "\n",
    "def get_pclass_dist(df):\n",
    "    \n",
    "    # Creating a dictionary for every passenger class count in every deck\n",
    "    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n",
    "    decks = df.columns.levels[0]    \n",
    "    \n",
    "    for deck in decks:\n",
    "        for pclass in range(1, 4):\n",
    "            try:\n",
    "                count = df[deck][pclass][0]\n",
    "                deck_counts[deck][pclass] = count \n",
    "            except KeyError:\n",
    "                deck_counts[deck][pclass] = 0\n",
    "                \n",
    "    df_decks = pd.DataFrame(deck_counts)    \n",
    "    deck_percentages = {}\n",
    "\n",
    "    # Creating a dictionary for every passenger class percentage in every deck\n",
    "    for col in df_decks.columns:\n",
    "        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n",
    "        \n",
    "    return deck_counts, deck_percentages\n",
    "\n",
    "def display_pclass_dist(percentages):\n",
    "    \n",
    "    df_percentages = pd.DataFrame(percentages).transpose()\n",
    "    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n",
    "    bar_count = np.arange(len(deck_names))  \n",
    "    bar_width = 0.85\n",
    "    \n",
    "    pclass1 = df_percentages[0]\n",
    "    pclass2 = df_percentages[1]\n",
    "    pclass3 = df_percentages[2]\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n",
    "    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n",
    "    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n",
    "\n",
    "    plt.xticks(bar_count, deck_names)\n",
    "    plt.xlabel('Deck', size=15, labelpad=20)\n",
    "    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n",
    "    \n",
    "    plt.tick_params(axis='x', labelsize=15)\n",
    "    plt.tick_params(axis='y', labelsize=15)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n",
    "    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)    \n",
    "    plt.show()    \n",
    "\n",
    "all_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\n",
    "display_pclass_dist(all_deck_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> * **100%** of **A**, **B** and **C** decks are 1st class passengers.\n",
    "* Deck **D** has **87%** 1st class and **13%** 2nd class passengers.\n",
    "* Deck **E** has **83%** 1st class, **10%** 2nd class and **7%** 3rd class passengers.\n",
    "* Deck **F** has **62%** 2nd class and **38%** 3rd class passengers.\n",
    "* **100%** of **G** deck are 3rd class passengers.\n",
    "* There is one person on the boat deck in the **T** cabin and he is a 1st class passenger. **T** cabin passenger has the closest resemblance to **A** deck passengers, so he is grouped in **A** deck.\n",
    "* Passengers labeled as **M** are the missing values in the `Cabin` feature. I don't think it is possible to find those passengers' real `Deck`. I decided to use **M** like a deck itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passenger in the T deck is changed to A\n",
    "idx = df_all[df_all['Deck'] == 'T'].index\n",
    "df_all.loc[idx, 'Deck'] = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n",
    "                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n",
    "\n",
    "def get_survived_dist(df):\n",
    "    \n",
    "    # Creating a dictionary for every survival count in every deck\n",
    "    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n",
    "    decks = df.columns.levels[0]    \n",
    "\n",
    "    for deck in decks:\n",
    "        for survive in range(0, 2):\n",
    "            surv_counts[deck][survive] = df[deck][survive][0]\n",
    "            \n",
    "    df_surv = pd.DataFrame(surv_counts)\n",
    "    surv_percentages = {}\n",
    "\n",
    "    for col in df_surv.columns:\n",
    "        surv_percentages[col] = [(count / df_surv[col].sum()) * 100 for count in df_surv[col]]\n",
    "        \n",
    "    return surv_counts, surv_percentages\n",
    "\n",
    "def display_surv_dist(percentages):\n",
    "    \n",
    "    df_survived_percentages = pd.DataFrame(percentages).transpose()\n",
    "    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n",
    "    bar_count = np.arange(len(deck_names))  \n",
    "    bar_width = 0.85    \n",
    "\n",
    "    not_survived = df_survived_percentages[0]\n",
    "    survived = df_survived_percentages[1]\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n",
    "    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n",
    "\n",
    "    plt.xticks(bar_count, deck_names)\n",
    "    plt.xlabel('Deck', size=15, labelpad=20)\n",
    "    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n",
    "    \n",
    "    plt.tick_params(axis='x', labelsize=15)\n",
    "    plt.tick_params(axis='y', labelsize=15)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n",
    "    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "all_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\n",
    "display_surv_dist(all_surv_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> As I suspected, every deck has different survival rates and we can't discard this information. Deck **B**, **C**, **D** and **E** have the highest survival rates. Those decks are mostly occupied by 1st class passengers. **M** has the lowest survival rate which is mostly occupied by 2nd and 3rd class passengers. To conclude cabins used by 1st class passengers have higher survival rates than cabins used by 2nd and 3rd class passengers. In my opinion **M** (Missing `Cabin` values) has the lowest survival rate because they couldn't retrieve the cabin data of the victims. That's why I believe, labeling that group as **M** is a reasonable way to handle the missing data. It is a unique group with same characteristics. `Deck` feature has high-cardinality right now, so some of the values are grouped with each other depending on their resemblance.\n",
    "* **A**, **B** and **C** decks are labeled as **ABC** because all of them have only 1st class passengers.\n",
    "* **D** and **E** decks are labeled as **DE** because both of them have similar passenger class distribution and same survival rate.\n",
    "* **F** and **G** decks are labeled as **FG** because of the former reason.\n",
    "* **M** deck doesn't need to be grouped with other decks because it is very different from others and has the lowest survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
    "df_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\n",
    "df_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FGM')\n",
    "\n",
    "df_all['Deck'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Cabin feature\n",
    "df_all.drop(['Cabin'], inplace=True, axis=1)\n",
    "\n",
    "df_train, df_test = divide_df(df_all)\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "for df in dfs:\n",
    "    display_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83ed6dacdc116214381364c2b456a97253e708ef"
   },
   "source": [
    ">> ### **1.3. Survival Distribution**\n",
    "* **38.38%** (342/891) of the training set is **Class 1**.\n",
    "* **61.62%** (549/891) of the training set is **Class 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c70aa13b7a552beb976574d52c1cd3da1cc1ee5c"
   },
   "outputs": [],
   "source": [
    "survived = df_train['Survived'].value_counts()[1]\n",
    "not_survived = df_train['Survived'].value_counts()[0]\n",
    "survived_per = survived / df_train.shape[0] * 100\n",
    "not_survived_per = not_survived / df_train.shape[0] * 100\n",
    "\n",
    "print('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\n",
    "print('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.countplot(df_train['Survived'])\n",
    "\n",
    "plt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\n",
    "plt.tick_params(axis='x', labelsize=13)\n",
    "plt.tick_params(axis='y', labelsize=13)\n",
    "plt.xlabel('Survival', size=15, labelpad=15)\n",
    "plt.ylabel('Passenger Count', size=15, labelpad=15)\n",
    "plt.title('Training Set Survival Distribution', size=15, y=1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **1.4. Correlations**\n",
    "* Features are highly correlated with each other and dependent to each other.\n",
    "* The highest correlation between features is **0.549500** in training set and **0.577147** in test set (between `Fare` and `Pclass`).\n",
    "* The other features are also highly correlated. There are **9** correlations in training set and **6** correlations in test set that are higher than **0.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\n",
    "df_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n",
    "\n",
    "df_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "df_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\n",
    "df_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set high correlations\n",
    "corr = df_train_corr_nd['Correlation Coefficient'] > 0.1\n",
    "df_train_corr_nd[corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set high correlations\n",
    "corr = df_test_corr_nd['Correlation Coefficient'] > 0.1\n",
    "df_test_corr_nd[corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n",
    "\n",
    "sns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n",
    "sns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n",
    "\n",
    "for i in range(2):    \n",
    "    axs[i].tick_params(axis='x', labelsize=14)\n",
    "    axs[i].tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "axs[0].set_title('Training Set Correlations', size=15)\n",
    "axs[1].set_title('Test Set Correlations', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **1.5. Survival Distribution in Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.5.1. Continuous Features**\n",
    "* Both of the continuous features, `Age` and `Fare`, have good split points and spikes for a decision tree.\n",
    "* Distribution of `Age` feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups.\n",
    "* In the distribution of `Fare` feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extreme outliers.\n",
    "* One potential problem for both features is, the distribution of the spikes are bumpy in training set, but smooth in test set. The machine learning algorithm may not able to generalize to test set because of this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['Age', 'Fare']\n",
    "surv = df_train['Survived'] == 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\n",
    "plt.subplots_adjust(right=1.5)\n",
    "\n",
    "for i, feature in enumerate(cont_features):    \n",
    "    # Distribution of survival in feature\n",
    "    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n",
    "    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n",
    "    \n",
    "    # Distribution of feature in dataset\n",
    "    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n",
    "    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n",
    "    \n",
    "    for j in range(2):        \n",
    "        axs[i][j].tick_params(axis='x', labelsize=20)\n",
    "        axs[i][j].tick_params(axis='y', labelsize=20)\n",
    "    \n",
    "    axs[0][i].legend(loc='upper right', prop={'size': 20})\n",
    "    axs[0][i].set_xlabel('')\n",
    "    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n",
    "    \n",
    "    axs[1][i].legend(loc='upper right', prop={'size': 20})\n",
    "    axs[1][i].set_xlabel('')\n",
    "    \n",
    "axs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\n",
    "axs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.5.2. Categorical Features**\n",
    "* Every categorical feature has at least one class with high not-survived rate. Those classes will be very helpful to predict whether the passenger is a survivor or victim.\n",
    "* Passengers boarded from **Southampton** has a lower survival rate unlike other ports.More than half of the passengers boarded from **Cherbourg** did survive.\n",
    "* `Parch` and `SibSp` features show that passengers with one family member has a higher survival rate.\n",
    "* Best categorical features are `Pclass` and `Sex` because they have the highest survived/not-survived ratio in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\n",
    "plt.subplots_adjust(right=1.5, top=1.25)\n",
    "\n",
    "for i, feature in enumerate(cat_features, 1):\n",
    "    \n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.countplot(x=feature, hue='Survived', data=df_train)\n",
    "    \n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n",
    "    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n",
    "    plt.ylabel('Passenger Count', size=20, labelpad=15)\n",
    "    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **1.5.3. Ticket Frequency**\n",
    "* There were too many `Ticket` values to analyze, so grouping them up by frequency makes things easier. **How is this feature different than family size (`Parch` and `SibSp`)?** Many passengers travelled along with a group. Those groups consist of friends, nannies, maids and etc. They werent counted as family, but they used the same ticket.\n",
    "* According to the graph, groups with **2**,**3** and **4** members had a higher rate of survival.\n",
    "* Passengers who travel alone has the lowest survival rate.\n",
    "* After **4** group members, the survival rate decreases drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count').values[:891]\n",
    "df_test['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count').values[891:]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12, 9))\n",
    "sns.countplot(x='Ticket_Frequency', hue='Survived', data=df_train)\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\n",
    "plt.xlabel('Ticket Frequency', size=15, labelpad=20)\n",
    "plt.ylabel('Passenger Count', size=15, labelpad=20)\n",
    "plt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **1.6. Conclusion**\n",
    "* Most of the features are correlated with each other. This relationship can be used to create new features with feature transformation and interaction. Target encoding could be very useful as well because of the high correlations with `Survived` feature.\n",
    "* Split points and spikes are very visible in the continuous feature. They can be captured easily with a decision tree, but neural networks may not be able to spot them.\n",
    "* Categorical features have very distinct classes with different survival rates. Some of those informations might also be combined to make new features.\n",
    "* Created **2** new features (`Deck` and `Ticket_Frequency`) and dropped **1** feature (`Cabin`) at the Exploratory Data Analysis part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = concat_df(df_train, df_test)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e14506a365afef0af44894e46642acf27ac2545f"
   },
   "source": [
    "> ## **2. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **2.1. Binning the Continuous Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **2.1.1. Fare**\n",
    "* `Fare` feature is positively skewed and survival rate is extremely high on the right end.\n",
    "* **13** quantile based bins are used for `Fare` feature. Even though it is too much, it provides decent amount of information gain.\n",
    "* The groups in the left side of the graph has the lowest survival rate and the groups in the right side of the graph has the highest survival rate. This high survival rate was not visible in the distribution graph.\n",
    "* There is also an unusual group **(15.742, 23.25]** with high survival rate that is captured in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(22, 9))\n",
    "sns.countplot(x='Fare', hue='Survived', data=df_all)\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=10)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\n",
    "plt.xlabel('Fare', size=15, labelpad=20)\n",
    "plt.ylabel('Passenger Count', size=15, labelpad=20)\n",
    "plt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **2.1.2. Age**\n",
    "* `Age` feature has a normal distribution with some spikes and bumps.\n",
    "* **10** quantile based bins are used for `Age` feature.\n",
    "* The first bin has the highest survival rate and 4th bin has the lowest survival rate. Those were the spikes in the distribution.\n",
    "* There is also an unusual group **(34.0, 40.0]** with high survival rate that is captured in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Age'] = pd.qcut(df_all['Age'], 10)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(22, 9))\n",
    "sns.countplot(x='Age', hue='Survived', data=df_all)\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\n",
    "plt.xlabel('Age', size=15, labelpad=20)\n",
    "plt.ylabel('Passenger Count', size=15, labelpad=20)\n",
    "plt.title('Count of Survival in {} Feature'.format('Age'), size=15, y=1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **2.2. Family Size**\n",
    "* `Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added to find the total size of families. Adding **1** at the end, is the passenger.\n",
    "* Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n",
    "* Family Size with **1** are labeled as **Alone**.\n",
    "* Family Size with **2**, **3** and **4** are labeled as **Small**.\n",
    "* Family Size with **5** and **6** are labeled as **Medium**.\n",
    "* Family Size with **7**, **8** and **11** are labeled as **Large**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\n",
    "plt.subplots_adjust(right=1.5)\n",
    "\n",
    "sns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\n",
    "sns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n",
    "axs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\n",
    "axs[0][1].set_title('Count of Survival in Family Size ', size=20, y=1.05)\n",
    "\n",
    "family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
    "df_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n",
    "\n",
    "sns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\n",
    "sns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n",
    "axs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\n",
    "axs[1][1].set_title('Count of Survival in Family Size After Grouping', size=20, y=1.05)\n",
    "\n",
    "for i in range(2):\n",
    "    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n",
    "    for j in range(2):\n",
    "        axs[i][j].tick_params(axis='x', labelsize=20)\n",
    "        axs[i][j].tick_params(axis='y', labelsize=20)\n",
    "        axs[i][j].set_xlabel('')\n",
    "        axs[i][j].set_ylabel('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **2.3. Title & Is Married**\n",
    "* `Title` is created by extracting the prefix before the `Name` column values.\n",
    "* `Is_Married` is a binary feature based on the **Mrs** title. **Mrs** title has the highest survival rate among other female titles. This title needs to be a feature itself because other female titles are grouped with each other.\n",
    "* According to the graph below, there are many titles which are occuring very few times. Some of those titles doesn't seem correct and they need to be replaced.\n",
    "* **Miss**, **Mrs**, **Ms**, **Mlle**, **Lady**, **Mme**, **the Countess**, **Dona** titles are replaced with **Miss/Mrs/Ms** because all of them are actually females. Values like **Mlle**, **Mme** and **Dona** are actually the name of the passengers, but they are classified as titles because `Name` feature is split by comma.\n",
    "* **Dr**, **Col**, **Major**, **Jonkheer**, **Capt**, **Sir**, **Don** and **Rev** titles are replaced with **Dr/Military/Noble/Clergy** because those passengers have similar characteristics.\n",
    "* **Master** is a unique title. It is given to male passengers below age **26**. They have the highest survival rate among all male groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "df_all['Is_Married'] = 0\n",
    "df_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n",
    "\n",
    "sns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n",
    "axs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n",
    "\n",
    "df_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "df_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n",
    "\n",
    "axs[0].tick_params(axis='x', labelsize=10)\n",
    "axs[1].tick_params(axis='x', labelsize=15)\n",
    "\n",
    "for i in range(2):    \n",
    "    axs[i].tick_params(axis='y', labelsize=15)\n",
    "\n",
    "sns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\n",
    "axs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **2.4. Survival Rate**\n",
    "* The first part of the feature, `Family_Survival_Rate`, is shared by [**@volhaleusha**](https://www.kaggle.com/volhaleusha) in this kernel [**Titanic: Tutorial, Encoding, Feature Eng, 81.8%**](https://www.kaggle.com/volhaleusha/titanic-tutorial-encoding-feature-eng-81-8), and it increases LB score significantly. However, it doesn't take passengers travelling with groups into consideration. Those groups doesn't necessarily have to be families. In order to overcome this problem, `Ticket_Survival_Rate` is also calculated and averaged with `Family_Survival_Rate`.\n",
    "* `extract_surname` function is used to extract the surname of the passenger from the `Name` feature. `Family` feature is created with the extracted surname. This is necessary for grouping the passengers in the same family. \n",
    "* Family survival rate is calculated from the families in training set since there is no `Survived` feature in test set.\n",
    "* A list of family names that are occuring in both training and test set, is created. The survival rate is calculated for families in that list and with more than 1 member, and stored in `Family_Survival_Rate` feature.\n",
    "* An extra binary feature `Family_Survival_Rate_NA` is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families' survival rate. This feature implies that family survival rate is not applicable to that passenger because there is no way to retrieve survival rate.\n",
    "* `Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are also created the same method.\n",
    "* `Ticket_Survival_Rate` and `Family_Survival_Rate` are averaged and become `Survival_Rate`, and `Ticket_Survival_Rate_NA` and `Family_Survival_Rate_NA` are averaged and become `Survival_Rate_NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surname(data):    \n",
    "    \n",
    "    families = []\n",
    "    \n",
    "    for i in range(len(data)):        \n",
    "        name = data.iloc[i]\n",
    "\n",
    "        if '(' in name:\n",
    "            name_no_bracket = name.split('(')[0] \n",
    "        else:\n",
    "            name_no_bracket = name\n",
    "            \n",
    "        family = name_no_bracket.split(',')[0]\n",
    "        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n",
    "        \n",
    "        for c in string.punctuation:\n",
    "            family = family.replace(c, '').strip()\n",
    "            \n",
    "        families.append(family)\n",
    "            \n",
    "    return families\n",
    "\n",
    "df_all['Family'] = extract_surname(df_all['Name'])\n",
    "df_train = df_all.loc[:890]\n",
    "df_test = df_all.loc[891:]\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "# Returning a list of families that are occuring in both training and test set\n",
    "non_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\n",
    "df_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\n",
    "\n",
    "family_rates = {}\n",
    "\n",
    "for i in range(len(df_family_survival_rate)):\n",
    "    # Checking a family exists in both training and test set, and has members more than 1\n",
    "    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n",
    "        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_survival_rate = np.mean(df_train['Survived'])\n",
    "\n",
    "train_family_survival_rate = []\n",
    "train_family_survival_rate_NA = []\n",
    "test_family_survival_rate = []\n",
    "test_family_survival_rate_NA = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    if df_train['Family'][i] in family_rates:\n",
    "        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n",
    "        train_family_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        train_family_survival_rate.append(mean_survival_rate)\n",
    "        train_family_survival_rate_NA.append(0)\n",
    "        \n",
    "for i in range(len(df_test)):\n",
    "    if df_test['Family'].iloc[i] in family_rates:\n",
    "        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n",
    "        test_family_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        test_family_survival_rate.append(mean_survival_rate)\n",
    "        test_family_survival_rate_NA.append(0)\n",
    "        \n",
    "df_train['Family_Survival_Rate'] = train_family_survival_rate\n",
    "df_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\n",
    "df_test['Family_Survival_Rate'] = test_family_survival_rate\n",
    "df_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a list of tickets that are occuring in both training and test set\n",
    "non_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n",
    "df_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n",
    "\n",
    "ticket_rates = {}\n",
    "\n",
    "for i in range(len(df_ticket_survival_rate)):\n",
    "    # Checking a ticket exists in both training and test set, and has members more than 1\n",
    "    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n",
    "        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n",
    "        \n",
    "train_ticket_survival_rate = []\n",
    "train_ticket_survival_rate_NA = []\n",
    "test_ticket_survival_rate = []\n",
    "test_ticket_survival_rate_NA = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    if df_train['Ticket'][i] in ticket_rates:\n",
    "        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n",
    "        train_ticket_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        train_ticket_survival_rate.append(mean_survival_rate)\n",
    "        train_ticket_survival_rate_NA.append(0)\n",
    "        \n",
    "for i in range(len(df_test)):\n",
    "    if df_test['Ticket'].iloc[i] in ticket_rates:\n",
    "        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n",
    "        test_ticket_survival_rate_NA.append(1)\n",
    "    else:\n",
    "        test_ticket_survival_rate.append(mean_survival_rate)\n",
    "        test_ticket_survival_rate_NA.append(0)\n",
    "        \n",
    "df_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\n",
    "df_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\n",
    "df_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\n",
    "df_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n",
    "    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b39a06d42b89d64f59f51a21dabdc5fe27fdcdaa"
   },
   "source": [
    ">> ### **2.5. Feature Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **2.5.1. Label Encoding the Non-Numerical Features**\n",
    "`Embarked`, `Sex`, `Deck` , `Title` and `Family_Size_Grouped` are object type, and `Age` and `Fare` features are category type. They are converted to numerical type with `LabelEncoder`. `LabelEncoder` basically labels the classes from **0** to **n**. This process is necessary for Machine Learning algorithms to learn from those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n",
    "\n",
    "for df in dfs:\n",
    "    for feature in non_numeric_features:        \n",
    "        df[feature] = LabelEncoder().fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> #### **2.5.2. One-Hot Encoding the Categorical Features**\n",
    "The categorical features (`Pclass`, `Sex`, `Deck`, `Embarked`, `Title`) are converted to one-hot encoded features with `OneHotEncoder`. `Age` and `Fare` features are not converted because they are ordinal unlike the former ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\n",
    "encoded_features = []\n",
    "\n",
    "for df in dfs:\n",
    "    for feature in cat_features:\n",
    "        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
    "        n = df[feature].nunique()\n",
    "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "        encoded_df.index = df.index\n",
    "        encoded_features.append(encoded_df)\n",
    "\n",
    "df_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\n",
    "df_test = pd.concat([df_test, *encoded_features[6:]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **2.6. Conclusion**\n",
    "* `Age` and `Fare` features are binned. Binning helped dealing with outliers and it revealed some homogeneous groups in those features.\n",
    "* `Family_Size` is created with the interaction between `Parch` and `SibSp` features.\n",
    "* `Name` feature is very useful. First, `Title` and `Is_Married` features are created from the title prefix in the name values. Second, `Family_Survival_Rate` and `Family_Survival_Rate_NA`  features are created by target encoding the surname of the passengers.\n",
    "* `Survival_Rate` feature is created by averaging the `Family_Survival_Rate` and `Ticket_Survival_Rate` features.\n",
    "* Finally, the non-numeric type features are label encoded and categorical features are one-hot encoded.\n",
    "* Created **5** new features (`Family_Size`, `Title`, `Is_Married`, `Survival_Rate` and `Survival_Rate_NA`) and dropped the useless features after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = concat_df(df_train, df_test)\n",
    "drop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n",
    "             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n",
    "            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n",
    "\n",
    "df_all.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b3d47212b52113b53a3e6d0d0a4ee08aa97b02f"
   },
   "source": [
    "> ## **3. Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=drop_cols)\n",
    "y_train = df_train['Survived']\n",
    "X_test = df_test.drop(columns=drop_cols)\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **3.1. Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(criterion='gini', \n",
    "                            n_estimators=1100,\n",
    "                            max_depth=5,\n",
    "                            min_samples_split=4,\n",
    "                            min_samples_leaf=5,\n",
    "                            max_features='auto',\n",
    "                            oob_score=True, \n",
    "                            random_state=SEED,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1) \n",
    "\n",
    "clf.fit(StandardScaler().fit_transform(X_train), y_train)\n",
    "print('RandomForestClassifier oob score: {}'.format(clf.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ### **3.2. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance = pd.DataFrame()\n",
    "df_feature_importance['feature'] = X_train.columns\n",
    "df_feature_importance['importance'] = clf.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "sns.barplot(x='importance', y='feature', data=df_feature_importance.sort_values(by='importance', ascending=False))\n",
    "plt.title('Random Forest Classifier Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(StandardScaler().fit_transform(X_test)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8154ae17f15be72790a702ca957d6bbf40029705"
   },
   "source": [
    ">> ### **3.3. Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e78fb513f18dc651e69e981e886b0d5445e4c4c4"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "submission_df['PassengerId'] = df_test['PassengerId']\n",
    "submission_df['Survived'] = y_pred\n",
    "submission_df.to_csv('submissions.csv', header=True, index=False)\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
