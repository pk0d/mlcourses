{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Введение в машинное обучение\n",
    "\n",
    "## Семинар #3\n",
    "\n",
    "### Екатерина Кондратьева\n",
    "\n",
    "ekaterina.kondrateva@skoltech.ru\n",
    "\n",
    "## Регуляризация в линейных моделях. Метод Ближайших Соседей (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Регуляризация в линейных моделях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация - наложение ограничения на решающае правило через нормы. Это нужно для борьбы с переобучением методом отбора признаков. \n",
    "\n",
    "Источники:\n",
    "1. https://github.com/esokolov/ml-course-hse/blob/master/2018-fall/lecture-notes/lecture03-linregr.pdf    \n",
    "2. http://www.machinelearning.ru/wiki/images/7/7e/VetrovSem11_LARS.pdf\n",
    "3. https://ru.coursera.org/lecture/supervised-learning/rieghuliarizatsiia-sR94Q\n",
    "4. https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear algebra\n",
    "import numpy as np\n",
    "#data structures\n",
    "import pandas as pd\n",
    "#ml models\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVR\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#beautiful plots\n",
    "import seaborn as sns\n",
    "#linear regression\n",
    "import statsmodels.api as sm\n",
    "#set style for plots\n",
    "sns.set_style('darkgrid')\n",
    "#off the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier     #KNN\n",
    "from sklearn.linear_model import LogisticRegression    #Logistic Regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the breast cancer data and few EDA\n",
    "cancer = load_breast_cancer()\n",
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression() # check the model params\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_train,y_train)))\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Какая регуляризация используется в модели?**\n",
    "\n",
    "`sklearn.linear_model.LogisticRegression()` не дает полный вывод параметров решающего правила логистической регресии.\n",
    "\n",
    "Полная справка доступна в библиотеке `statsmodels.api`, однако в ней не поддреживаются некоторые регуляризации и их нужно прописывать формульно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = cancer.data.shape[1]\n",
    "plt.barh(range(n_feature), abs(log_reg.coef_[0]), align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#together with the intercept\n",
    "k = X_train.shape[1]\n",
    "#total number of observations\n",
    "n = X_train.shape[0]\n",
    "#degrees of freedom for the model:\n",
    "df_model = k - 1\n",
    "#degrees of freedom of the error:\n",
    "df_error = n - k\n",
    "print(' the number of parameters to estimate: {} \\n total number of observations: {} \\n degrees of freedom of the model: {} \\n degrees of freedom of the errors: {}'\\\n",
    "      .format(k, n, df_model, df_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the constant term to the data\n",
    "X = sm.add_constant(X_train)\n",
    "#define the model\n",
    "model = sm.OLS(y_train, X)\n",
    "#fit the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = cancer.data.shape[1]\n",
    "plt.barh(range(n_feature), results.params[1:], align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Lasso regularisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l1') # check the model params\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_train,y_train)))\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = cancer.data.shape[1]\n",
    "plt.barh(range(n_feature), log_reg.coef_[0], align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2') # check the model params\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_train,y_train)))\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_test,y_test)))\n",
    "\n",
    "#Feature Importance\n",
    "n_feature = cancer.data.shape[1]\n",
    "plt.barh(range(n_feature), log_reg.coef_[0], align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l1', C=0.01 ) # check the model params\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_train,y_train)))\n",
    "print('Accuracy on the training set: {:.3f}'.format(log_reg.score(X_test,y_test)))\n",
    "\n",
    "#Feature Importance\n",
    "n_feature = cancer.data.shape[1]\n",
    "plt.barh(range(n_feature), log_reg.coef_[0], align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net (L1 & L2 regularisation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit_regularized() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "n_feature = cancer.data.shape[1]\n",
    "plt.barh(range(n_feature), results.params[1:], align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(range(n_feature), abs(results.params[1:]), align='center') # to look at the coefs\n",
    "plt.yticks(np.arange(n_feature), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопросы:\n",
    "\n",
    "- Помогла ли l1 жесткася регуляризация избежать переобучения?\n",
    "- Переобучилась ли модель на нескольких характеристиках?\n",
    "- Как это проверить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "`Sklearn`:\n",
    "<a href='http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html'>Regressor</a>\n",
    "<a href='http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html'>Classifier</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:18:35.634420Z",
     "start_time": "2018-08-01T16:18:32.928271Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Тренируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:18:43.909285Z",
     "start_time": "2018-08-01T16:18:43.799059Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "#######\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:22:07.852739Z",
     "start_time": "2018-08-01T15:22:07.843104Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Метрики и ядерные функции:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Есть разные метрики: <a href='http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html'>sklearn.neighbors.DistanceMetric¶</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:22:07.862395Z",
     "start_time": "2018-08-01T15:22:07.856456Z"
    }
   },
   "outputs": [],
   "source": [
    "# “euclidean”   EuclideanDistance    sqrt(sum((x - y)^2))\n",
    "# “manhattan”   ManhattanDistance    sum(|x - y|)\n",
    "# “chebyshev”   ChebyshevDistance    max(|x - y|)\n",
    "# “minkowski”   MinkowskiDistance    sum(|x - y|^p)^(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:37:13.800932Z",
     "start_time": "2018-08-01T16:37:11.260639Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pair=[0, 1]\n",
    "X = iris.data[:, [0, 1]]\n",
    "y = iris.target\n",
    "\n",
    "n_classes = 3\n",
    "plot_colors = ['g', 'gold', 'black']\n",
    "plot_step = 0.005\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=10, \n",
    "                           metric='chebyshev', \n",
    "                           weights=gaussian_kernel,\n",
    "                           p=2).fit(X, y)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap='Accent')\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:42:19.129848Z",
     "start_time": "2018-08-01T16:42:18.968192Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "# y = np.piecewise(X.flatten(), \n",
    "#                  [X.flatten() < 3, X.flatten() >= 3], [-1, 1]).ravel()\n",
    "y[::2] += 1 * (0.5 - rng.rand(40))\n",
    "\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "clf = KNeighborsRegressor(n_neighbors=10, \n",
    "                          metric='chebyshev', \n",
    "                          weights=gaussian_kernel).fit(X, y)\n",
    "y_ = clf.predict(X_test)\n",
    "plt.scatter(X, y, c='darkorange', label='data')\n",
    "plt.plot(X_test, y_, c='cornflowerblue', label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проклятие размерности: \n",
    "http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#the-curse-of-dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Best Value Of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Standardize features\n",
    "X_std = standardizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a KNN classifier with 5 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', n_jobs=-1).fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([('standardizer', standardizer), ('knn', knn)])\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [{'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search \n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0).fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best neighborhood size (k)\n",
    "clf.best_estimator_.get_params()['knn__n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#----------KNN Classifier \n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#try KNN for diffrent k nearest neighbor from 1 to 15\n",
    "neighbors_setting = range(1,15)\n",
    "\n",
    "for n_neighbors in neighbors_setting:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train,y_train)\n",
    "    training_accuracy.append(knn.score(X_train, y_train))\n",
    "    test_accuracy.append(knn.score(X_test, y_test))\n",
    " \n",
    "plt.plot(neighbors_setting,training_accuracy, label='Accuracy of the training set')\n",
    "plt.plot(neighbors_setting,test_accuracy, label='Accuracy of the test set')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.legend()\n",
    "\n",
    "#by looking at plot, best result accurs when n_neighbors is 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы: \n",
    "- kNN - очень популярный алгоритм в тех случаях, когда объекты сравнимы и имеет место \"похожесть\" объектов.\n",
    "- kNN сильно зависит от метрики. А значит и от масштаба признаков. Перед применением нужно привести признаки к одной шкале\n",
    "- kNN плохо воспринимает большое (>100) количество признаков, т.к. объекты оказываются одинаково отдалены друг от друга в таких пространствах "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 1: Применить классификацию kNN на датасете breast cancer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
